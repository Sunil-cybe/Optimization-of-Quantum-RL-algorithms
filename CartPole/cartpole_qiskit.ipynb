{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on : https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'done', 'next_state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(\n",
    "            *[self.memory[idx] for idx in indices])\n",
    "        states = torch.from_numpy(np.array(states)).to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).to(device)\n",
    "        rewards = torch.from_numpy(np.array(rewards,\n",
    "                                            dtype=np.float32)).to(device)\n",
    "        dones = torch.from_numpy(np.array(dones, dtype=np.int32)).to(device)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).to(device)\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on : https://github.com/djbyrne/core_rl/blob/master/algos/dqn/model.py\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 net,\n",
    "                 action_space=None,\n",
    "                 exploration_initial_eps=None,\n",
    "                 exploration_decay=None,\n",
    "                 exploration_final_eps=None):\n",
    "\n",
    "        self.net = net\n",
    "        self.action_space = action_space\n",
    "        self.exploration_initial_eps = exploration_initial_eps\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.epsilon = 0.\n",
    "\n",
    "    def __call__(self, state, device=torch.device('cpu')):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_action(state, device)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_random_action(self):\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def get_action(self, state, device=torch.device('cpu')):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor([state])\n",
    "\n",
    "        if device.type != 'cpu':\n",
    "            state = state.cuda(device)\n",
    "\n",
    "        q_values = self.net.eval()(state)\n",
    "        _, action = torch.max(q_values, dim=1)\n",
    "        return int(action.item())\n",
    "\n",
    "    def update_epsilon(self, step):\n",
    "        self.epsilon = max(\n",
    "            self.exploration_final_eps, self.exploration_final_eps +\n",
    "            (self.exploration_initial_eps - self.exploration_final_eps) *\n",
    "            self.exploration_decay**step)\n",
    "        return self.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 net,\n",
    "                 target_net,\n",
    "                 gamma,\n",
    "                 learning_rate,\n",
    "                 batch_size,\n",
    "                 exploration_initial_eps,\n",
    "                 exploration_decay,\n",
    "                 exploration_final_eps,\n",
    "                 train_freq,\n",
    "                 target_update_interval,\n",
    "                 buffer_size,\n",
    "                 learning_rate_input=None,\n",
    "                 learning_rate_output=None,\n",
    "                 loss_func='MSE',\n",
    "                 optim_class='RMSprop',\n",
    "                 device='cpu',\n",
    "                 logging=False):\n",
    "\n",
    "        assert loss_func in ['MSE', 'L1', 'SmoothL1'\n",
    "                            ], \"Supported losses : ['MSE', 'L1', 'SmoothL1']\"\n",
    "        assert optim_class in [\n",
    "            'SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta'\n",
    "        ], \"Supported optimizers : ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\"\n",
    "        assert device in ['auto', 'cpu', 'cuda:0'\n",
    "                         ], \"Supported devices : ['auto', 'cpu', 'cuda:0']\"\n",
    "\n",
    "        self.env = env\n",
    "        self.net = net\n",
    "        self.target_net = target_net\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.exploration_initial_eps = exploration_initial_eps\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.train_freq = train_freq\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.buffer_size = buffer_size\n",
    "        self.learning_rate_input = learning_rate_input\n",
    "        self.learning_rate_output = learning_rate_output\n",
    "        self.loss_func = loss_func\n",
    "        self.optim_class = optim_class\n",
    "        self.device = device\n",
    "        self.logging = logging\n",
    "\n",
    "        self.build()\n",
    "        self.reset()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # set networks\n",
    "        if self.device == \"auto\":\n",
    "            self.device = torch.device(\n",
    "                \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(self.device)\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.target_net = self.target_net.to(self.device)\n",
    "\n",
    "        # set loss\n",
    "        self.loss_func = getattr(nn, self.loss_func + 'Loss')()\n",
    "\n",
    "        # set optimizer\n",
    "        optim_class = getattr(optim, self.optim_class)\n",
    "        params = []\n",
    "        params.append({'params': self.net.q_layers.parameters()})\n",
    "        if hasattr(self.net, 'w_input') and self.net.w_input is not None:\n",
    "            lr_input = self.learning_rate_input if self.learning_rate_input is not None else self.learning_rate\n",
    "            params.append({'params': self.net.w_input, 'lr': lr_input})\n",
    "        if hasattr(self.net, 'w_output') and self.net.w_output is not None:\n",
    "            lr_output = self.learning_rate_output if self.learning_rate_output is not None else self.learning_rate\n",
    "            params.append({'params': self.net.w_output, 'lr': lr_output})\n",
    "        self.opt = optim_class(params, lr=self.learning_rate)\n",
    "\n",
    "        # set agent\n",
    "        self.agent = Agent(self.net, self.env.action_space,\n",
    "                           self.exploration_initial_eps, self.exploration_decay,\n",
    "                           self.exploration_final_eps)\n",
    "\n",
    "        # set memory\n",
    "        self.memory = ReplayMemory(self.buffer_size)\n",
    "\n",
    "        # set loggers\n",
    "\n",
    "        if self.logging:\n",
    "            exp_name = datetime.now().strftime(\"DQN-%d_%m_%Y-%H_%M_%S\")\n",
    "            if not os.path.exists('./logs/'):\n",
    "                os.makedirs('./logs/')\n",
    "            self.log_dir = './logs/{}/'.format(exp_name)\n",
    "            os.makedirs(self.log_dir)\n",
    "            self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "    def reset(self):\n",
    "        self.global_step = 0\n",
    "        self.episode_count = 0\n",
    "        self.env.seed(123)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        state = self.env.reset()\n",
    "        while len(self.memory) < self.buffer_size:\n",
    "            action = self.agent.get_random_action()\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.memory.push(state, action, reward, done, next_state)\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    def update_net(self):\n",
    "\n",
    "        self.net.train()\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "        # sample transitions\n",
    "        states, actions, rewards, dones, next_states = self.memory.sample(\n",
    "            self.batch_size, self.device)\n",
    "\n",
    "        # compute q-values\n",
    "        state_action_values = self.net(states)\n",
    "        state_action_values = state_action_values.gather(\n",
    "            1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # compute target q-values\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states)\n",
    "            next_state_values = next_state_values.max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (1 - dones) * next_state_values.to(\n",
    "            self.device) * self.gamma + rewards\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_func(state_action_values, expected_state_action_values)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "    def train_step(self):\n",
    "        episode_epsilon = self.agent.update_epsilon(self.episode_count)\n",
    "        episode_steps = 0\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # take action\n",
    "            action = self.agent(state, self.device)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "            # update memory\n",
    "            self.memory.push(state, action, reward, done, next_state)\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            # optimize net\n",
    "            if self.global_step % self.train_freq == 0:\n",
    "                loss = self.update_net()\n",
    "                episode_loss.append(loss)\n",
    "\n",
    "            # update target net\n",
    "            if self.global_step % self.target_update_interval == 0:\n",
    "                self.update_target_net()\n",
    "\n",
    "            self.global_step += 1\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "        self.episode_count += 1\n",
    "        if len(episode_loss) > 0:\n",
    "            episode_loss = np.mean(episode_loss)\n",
    "        else:\n",
    "            episode_loss = 0.\n",
    "        return {\n",
    "            'steps': episode_steps,\n",
    "            'loss': episode_loss,\n",
    "            'reward': episode_reward,\n",
    "            'epsilon': episode_epsilon\n",
    "        }\n",
    "\n",
    "    def test_step(self, n_eval_episodes):\n",
    "        episode_steps = []\n",
    "        episode_reward = []\n",
    "\n",
    "        for _ in range(n_eval_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_steps.append(0)\n",
    "            episode_reward.append(0)\n",
    "            while not done:\n",
    "                action = self.agent.get_action(state, self.device)\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                episode_steps[-1] += 1\n",
    "                episode_reward[-1] += reward\n",
    "\n",
    "        episode_steps = np.mean(episode_steps)\n",
    "        episode_reward = np.mean(episode_reward)\n",
    "        return {'steps': episode_steps, 'reward': episode_reward}\n",
    "\n",
    "    def learn(self,\n",
    "              total_episodes,\n",
    "              n_eval_episodes=5,\n",
    "              log_train_freq=-1,\n",
    "              log_eval_freq=-1,\n",
    "              log_ckp_freq=-1):\n",
    "\n",
    "        # Stats\n",
    "        postfix_stats = {}\n",
    "        with tqdm(range(total_episodes), desc=\"DQN\",\n",
    "                  unit=\"episode\") as tepisodes:\n",
    "\n",
    "            for t in tepisodes:\n",
    "\n",
    "                # train dqn\n",
    "                train_stats = self.train_step()\n",
    "\n",
    "                # update train stats\n",
    "                postfix_stats['train/reward'] = train_stats['reward']\n",
    "                postfix_stats['train/steps'] = train_stats['steps']\n",
    "\n",
    "                if t % log_eval_freq == 0:\n",
    "\n",
    "                    # test dqn\n",
    "                    test_stats = self.test_step(n_eval_episodes)\n",
    "\n",
    "                    # update test stats\n",
    "                    postfix_stats['test/reward'] = test_stats['reward']\n",
    "                    postfix_stats['test/steps'] = test_stats['steps']\n",
    "\n",
    "                if self.logging and (t % log_train_freq == 0):\n",
    "                    for key, item in train_stats.items():\n",
    "                        self.writer.add_scalar('train/' + key, item, t)\n",
    "\n",
    "                if self.logging and (t % log_eval_freq == 0):\n",
    "                    for key, item in test_stats.items():\n",
    "                        self.writer.add_scalar('test/' + key, item, t)\n",
    "\n",
    "                if self.logging and (t % log_ckp_freq == 0):\n",
    "                    torch.save(self.net.state_dict(),\n",
    "                               self.log_dir + 'episode_{}.pt'.format(t))\n",
    "\n",
    "                # update progress bar\n",
    "                tepisodes.set_postfix(postfix_stats)\n",
    "\n",
    "            if self.logging and (log_ckp_freq > 0):\n",
    "                torch.save(self.net.state_dict(),\n",
    "                           self.log_dir + 'episode_final.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
