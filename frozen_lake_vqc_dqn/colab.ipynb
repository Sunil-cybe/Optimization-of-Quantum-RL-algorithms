{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dell\\miniconda3\\envs\\gyenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "# add common dqn utils\n",
    "sys.path.append('..')\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import yaml\n",
    "#from common.trainer import Trainer\n",
    "#from common.wrappers import BinaryWrapper\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--n_layers\", default=5, type=int)\n",
    "parser.add_argument(\"--gamma\", default=0.9, type=float)\n",
    "parser.add_argument(\"--lr\", default=0.01, type=float)\n",
    "parser.add_argument(\"--batch_size\", default=16, type=int)\n",
    "parser.add_argument(\"--eps_init\", default=1., type=float)\n",
    "parser.add_argument(\"--eps_decay\", default=0.99, type=int)\n",
    "parser.add_argument(\"--eps_min\", default=0.01, type=float)\n",
    "parser.add_argument(\"--train_freq\", default=5, type=int)\n",
    "parser.add_argument(\"--target_freq\", default=10, type=int)\n",
    "parser.add_argument(\"--memory\", default=10000, type=int)\n",
    "parser.add_argument(\"--loss\", default='SmoothL1', type=str)\n",
    "parser.add_argument(\"--optimizer\", default='Adam', type=str)\n",
    "parser.add_argument(\"--total_episodes\", default=1000, type=int)\n",
    "parser.add_argument(\"--n_eval_episodes\", default=5, type=int)\n",
    "parser.add_argument(\"--logging\", default=True, type=bool)\n",
    "parser.add_argument(\"--log_train_freq\", default=1, type=int)\n",
    "parser.add_argument(\"--log_eval_freq\", default=20, type=int)\n",
    "parser.add_argument(\"--log_ckp_freq\", default=50, type=int)\n",
    "parser.add_argument(\"--device\", default='auto', type=str)\n",
    "args = parser.parse_args(args = [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 net,\n",
    "                 action_space=None,\n",
    "                 exploration_initial_eps=None,\n",
    "                 exploration_decay=None,\n",
    "                 exploration_final_eps=None):\n",
    "\n",
    "        self.net = net\n",
    "        self.action_space = action_space\n",
    "        self.exploration_initial_eps = exploration_initial_eps\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.epsilon = 0.\n",
    "\n",
    "    def __call__(self, state, device=torch.device('cpu')):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.get_random_action()\n",
    "        else:\n",
    "            action = self.get_action(state, device)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_random_action(self):\n",
    "        action = self.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def get_action(self, state, device=torch.device('cpu')):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor([state])\n",
    "\n",
    "        if device.type != 'cpu':\n",
    "            state = state.cuda(device)\n",
    "\n",
    "        q_values = self.net.eval()(state)\n",
    "        _, action = torch.max(q_values, dim=1)\n",
    "        return int(action.item())\n",
    "\n",
    "    def update_epsilon(self, step):\n",
    "        self.epsilon = max(\n",
    "            self.exploration_final_eps, self.exploration_final_eps +\n",
    "            (self.exploration_initial_eps - self.exploration_final_eps) *\n",
    "            self.exploration_decay**step)\n",
    "        return self.epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, agent, n_eval_episodes):\n",
    "    episode_steps = []\n",
    "    episode_reward = []\n",
    "\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_steps.append(0)\n",
    "        episode_reward.append(0)\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_steps[-1] += 1\n",
    "            episode_reward[-1] += reward\n",
    "\n",
    "    episode_steps = np.mean(episode_steps)\n",
    "    episode_reward = np.mean(episode_reward)\n",
    "    return {'steps': episode_steps, 'reward': episode_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'reward', 'done', 'next_state'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.buffer_size:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(\n",
    "            *[self.memory[idx] for idx in indices])\n",
    "        states = torch.from_numpy(np.array(states)).to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).to(device)\n",
    "        rewards = torch.from_numpy(np.array(rewards,\n",
    "                                            dtype=np.float32)).to(device)\n",
    "        dones = torch.from_numpy(np.array(dones, dtype=np.int32)).to(device)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).to(device)\n",
    "        return states, actions, rewards, dones, next_states\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 net,\n",
    "                 target_net,\n",
    "                 gamma,\n",
    "                 learning_rate,\n",
    "                 batch_size,\n",
    "                 exploration_initial_eps,\n",
    "                 exploration_decay,\n",
    "                 exploration_final_eps,\n",
    "                 train_freq,\n",
    "                 target_update_interval,\n",
    "                 buffer_size,\n",
    "                 learning_rate_input=None,\n",
    "                 learning_rate_output=None,\n",
    "                 loss_func='MSE',\n",
    "                 optim_class='RMSprop',\n",
    "                 device='auto',\n",
    "                 logging=False):\n",
    "\n",
    "        assert loss_func in ['MSE', 'L1', 'SmoothL1'\n",
    "                             ], \"Supported losses : ['MSE', 'L1', 'SmoothL1']\"\n",
    "        assert optim_class in [\n",
    "            'SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta'\n",
    "        ], \"Supported optimizers : ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\"\n",
    "        assert device in ['auto', 'cpu', 'cuda:0'\n",
    "                          ], \"Supported devices : ['auto', 'cpu', 'cuda:0']\"\n",
    "\n",
    "        self.env = env\n",
    "        self.net = net\n",
    "        self.target_net = target_net\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.exploration_initial_eps = exploration_initial_eps\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_final_eps = exploration_final_eps\n",
    "        self.train_freq = train_freq\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.buffer_size = buffer_size\n",
    "        self.learning_rate_input = learning_rate_input\n",
    "        self.learning_rate_output = learning_rate_output\n",
    "        self.loss_func = loss_func\n",
    "        self.optim_class = optim_class\n",
    "        self.device = device\n",
    "        self.logging = logging\n",
    "\n",
    "        self.build()\n",
    "        self.reset()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        # set networks\n",
    "        if self.device == \"auto\":\n",
    "            self.device = torch.device(\n",
    "                \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = torch.device(self.device)\n",
    "        self.net = self.net.to(self.device)\n",
    "        self.target_net = self.target_net.to(self.device)\n",
    "\n",
    "        # set loss\n",
    "        self.loss_func = getattr(nn, self.loss_func + 'Loss')()\n",
    "\n",
    "        # set optimizer\n",
    "        optim_class = getattr(optim, self.optim_class)\n",
    "        params = []\n",
    "        params.append({'params': self.net.q_layers.parameters()})\n",
    "        if hasattr(self.net, 'w_input') and self.net.w_input is not None:\n",
    "            lr_input = self.learning_rate_input if self.learning_rate_input is not None else self.learning_rate\n",
    "            params.append({'params': self.net.w_input, 'lr': lr_input})\n",
    "        if hasattr(self.net, 'w_output') and self.net.w_output is not None:\n",
    "            lr_output = self.learning_rate_output if self.learning_rate_output is not None else self.learning_rate\n",
    "            params.append({'params': self.net.w_output, 'lr': lr_output})\n",
    "        self.opt = optim_class(params, lr=self.learning_rate)\n",
    "\n",
    "        # set agent\n",
    "        self.agent = Agent(self.net, self.env.action_space,\n",
    "                           self.exploration_initial_eps,\n",
    "                           self.exploration_decay, self.exploration_final_eps)\n",
    "\n",
    "        # set memory\n",
    "        self.memory = ReplayMemory(self.buffer_size)\n",
    "\n",
    "        # set loggers\n",
    "\n",
    "        if self.logging:\n",
    "            exp_name = datetime.now().strftime(\"DQN-%d_%m_%Y-%H_%M_%S\")\n",
    "            if not os.path.exists('./logs/'):\n",
    "                os.makedirs('./logs/')\n",
    "            self.log_dir = './logs/{}/'.format(exp_name)\n",
    "            os.makedirs(self.log_dir)\n",
    "            self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "\n",
    "    def reset(self):\n",
    "        self.global_step = 0\n",
    "        self.episode_count = 0\n",
    "        #self.env.seed(123)\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        state, _ = self.env.reset()\n",
    "        while len(self.memory) < self.buffer_size:\n",
    "            action = self.agent.get_random_action()\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            self.memory.push(state, action, reward, done, next_state)\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    def update_net(self):\n",
    "\n",
    "        self.net.train()\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "        # sample transitions\n",
    "        states, actions, rewards, dones, next_states = self.memory.sample(\n",
    "            self.batch_size, self.device)\n",
    "\n",
    "        # compute q-values\n",
    "        state_action_values = self.net(states)\n",
    "        actions = actions.to(torch.int64)\n",
    "        state_action_values = state_action_values.gather(\n",
    "            1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # compute target q-values\n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states)\n",
    "            next_state_values = next_state_values.max(1)[0].detach()\n",
    "\n",
    "        expected_state_action_values = (1 - dones) * next_state_values.to(\n",
    "            self.device) * self.gamma + rewards\n",
    "\n",
    "        # compute loss\n",
    "        loss = self.loss_func(state_action_values,\n",
    "                              expected_state_action_values)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "    def train_step(self):\n",
    "        episode_epsilon = self.agent.update_epsilon(self.episode_count)\n",
    "        episode_steps = 0\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # take action\n",
    "            action = self.agent(state, self.device)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "            # update memory\n",
    "            self.memory.push(state, action, reward, done, next_state)\n",
    "\n",
    "            # update state\n",
    "            state = next_state\n",
    "\n",
    "            # optimize net\n",
    "            if self.global_step % self.train_freq == 0:\n",
    "                loss = self.update_net()\n",
    "                episode_loss.append(loss)\n",
    "\n",
    "            # update target net\n",
    "            if self.global_step % self.target_update_interval == 0:\n",
    "                self.update_target_net()\n",
    "\n",
    "            self.global_step += 1\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "        self.episode_count += 1\n",
    "        if len(episode_loss) > 0:\n",
    "            episode_loss = np.mean(episode_loss)\n",
    "        else:\n",
    "            episode_loss = 0.\n",
    "        return {\n",
    "            'steps': episode_steps,\n",
    "            'loss': episode_loss,\n",
    "            'reward': episode_reward,\n",
    "            'epsilon': episode_epsilon\n",
    "        }\n",
    "\n",
    "    def test_step(self, n_eval_episodes):\n",
    "        episode_steps = []\n",
    "        episode_reward = []\n",
    "\n",
    "        for _ in range(n_eval_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            done = False\n",
    "            episode_steps.append(0)\n",
    "            episode_reward.append(0)\n",
    "            while not done:\n",
    "                action = self.agent.get_action(state, self.device)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                state = next_state\n",
    "                episode_steps[-1] += 1\n",
    "                episode_reward[-1] += reward\n",
    "\n",
    "        episode_steps = np.mean(episode_steps)\n",
    "        episode_reward = np.mean(episode_reward)\n",
    "        return {'steps': episode_steps, 'reward': episode_reward}\n",
    "\n",
    "    def learn(self,\n",
    "              total_episodes,\n",
    "              n_eval_episodes=5,\n",
    "              log_train_freq=-1,\n",
    "              log_eval_freq=-1,\n",
    "              log_ckp_freq=-1):\n",
    "\n",
    "        # Stats\n",
    "        postfix_stats = {}\n",
    "        with tqdm(range(total_episodes), desc=\"DQN\",\n",
    "                  unit=\"episode\") as tepisodes:\n",
    "\n",
    "            for t in tepisodes:\n",
    "\n",
    "                # train dqn\n",
    "                train_stats = self.train_step()\n",
    "\n",
    "                # update train stats\n",
    "                postfix_stats['train/reward'] = train_stats['reward']\n",
    "                postfix_stats['train/steps'] = train_stats['steps']\n",
    "\n",
    "                if t % log_eval_freq == 0:\n",
    "\n",
    "                    # test dqn\n",
    "                    test_stats = self.test_step(n_eval_episodes)\n",
    "\n",
    "                    # update test stats\n",
    "                    postfix_stats['test/reward'] = test_stats['reward']\n",
    "                    postfix_stats['test/steps'] = test_stats['steps']\n",
    "\n",
    "                if self.logging and (t % log_train_freq == 0):\n",
    "                    for key, item in train_stats.items():\n",
    "                        self.writer.add_scalar('train/' + key, item, t)\n",
    "\n",
    "                if self.logging and (t % log_eval_freq == 0):\n",
    "                    for key, item in test_stats.items():\n",
    "                        self.writer.add_scalar('test/' + key, item, t)\n",
    "\n",
    "                if self.logging and (t % log_ckp_freq == 0):\n",
    "                    torch.save(self.net.state_dict(),\n",
    "                               self.log_dir + 'episode_{}.pt'.format(t))\n",
    "\n",
    "                # update progress bar\n",
    "                tepisodes.set_postfix(postfix_stats)\n",
    "\n",
    "            if self.logging and (log_ckp_freq > 0):\n",
    "                torch.save(self.net.state_dict(),\n",
    "                           self.log_dir + 'episode_final.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.spaces\n",
    "\n",
    "class BinaryWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(BinaryWrapper, self).__init__(env)\n",
    "        self.bits = int(np.ceil(np.log2(env.observation_space.n)))\n",
    "        self.observation_space = gym.spaces.MultiBinary(self.bits)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        binary = map(float, \"{0:b}\".format(int(obs)).zfill(self.bits))\n",
    "        return np.array(list(binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DQN:   0%|          | 0/1000 [00:00<?, ?episode/s]C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_20720\\3605814501.py:30: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  state = torch.tensor([state])\n",
      "DQN:  14%|█▍        | 142/1000 [07:33<1:02:24,  4.36s/episode, train/reward=0, train/steps=19, test/reward=0.2, test/steps=14.6]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def encode(n_qubits, inputs):\n",
    "    for wire in range(n_qubits):\n",
    "        qml.RX(inputs[wire], wires=wire)\n",
    "\n",
    "\n",
    "def layer(n_qubits, y_weight, z_weight):\n",
    "    for wire, y_weight in enumerate(y_weight):\n",
    "        qml.RY(y_weight, wires=wire)\n",
    "    for wire, z_weight in enumerate(z_weight):\n",
    "        qml.RZ(z_weight, wires=wire)\n",
    "    for wire in range(n_qubits):\n",
    "        qml.CZ(wires=[wire, (wire + 1) % n_qubits])\n",
    "\n",
    "\n",
    "def measure(n_qubits):\n",
    "    return [qml.expval(qml.PauliZ(wire)) for wire in range(n_qubits)]\n",
    "\n",
    "\n",
    "def get_model(n_qubits, n_layers, data_reupload):\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    shapes = {\n",
    "        \"y_weights\": (n_layers, n_qubits),\n",
    "        \"z_weights\": (n_layers, n_qubits)\n",
    "    }\n",
    "\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def circuit(inputs, y_weights, z_weights):\n",
    "        for layer_idx in range(n_layers):\n",
    "            if (layer_idx == 0) or data_reupload:\n",
    "                encode(n_qubits, inputs)\n",
    "            layer(n_qubits, y_weights[layer_idx], z_weights[layer_idx])\n",
    "        return measure(n_qubits)\n",
    "\n",
    "    model = qml.qnn.TorchLayer(circuit, shapes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super(QuantumNet, self).__init__()\n",
    "        self.n_qubits = 4\n",
    "        self.n_actions = 4\n",
    "        self.q_layers = get_model(n_qubits=self.n_qubits,\n",
    "                                  n_layers=n_layers,\n",
    "                                  data_reupload=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs * np.pi\n",
    "        outputs = self.q_layers(inputs)\n",
    "        outputs = (1 + outputs) / 2\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Environment\n",
    "    env_name = 'FrozenLake-v1'\n",
    "    env = gym.make(env_name, is_slippery = False)\n",
    "    env = BinaryWrapper(env)\n",
    "\n",
    "    # Networks\n",
    "    net = QuantumNet(args.n_layers)\n",
    "    target_net = QuantumNet(args.n_layers)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(env,\n",
    "                      net,\n",
    "                      target_net,\n",
    "                      gamma=args.gamma,\n",
    "                      learning_rate=args.lr,\n",
    "                      batch_size=args.batch_size,\n",
    "                      exploration_initial_eps=args.eps_init,\n",
    "                      exploration_decay=args.eps_decay,\n",
    "                      exploration_final_eps=args.eps_min,\n",
    "                      train_freq=args.train_freq,\n",
    "                      target_update_interval=args.target_freq,\n",
    "                      buffer_size=args.memory,\n",
    "                      loss_func=args.loss,\n",
    "                      optim_class=args.optimizer,\n",
    "                      device=args.device,\n",
    "                      logging=args.logging)\n",
    "\n",
    "    #if args.logging:\n",
    "    #    with open(trainer.log_dir + 'config.yaml', 'w') as f:\n",
    "    #        yaml.safe_dump(args.__dict__, f, indent=2)\n",
    "\n",
    "    trainer.learn(args.total_episodes,\n",
    "                  n_eval_episodes=args.n_eval_episodes,\n",
    "                  log_train_freq=args.log_train_freq,\n",
    "                  log_eval_freq=args.log_eval_freq,\n",
    "                  log_ckp_freq=args.log_ckp_freq)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
