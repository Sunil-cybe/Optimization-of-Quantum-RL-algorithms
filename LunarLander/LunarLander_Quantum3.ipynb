{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKJH7NQui-Lg",
        "outputId": "08190368-98e1-40a8-d0ff-4ea17c7da169"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'%%bash\\npip3 install swig\\npip3 install gymnasium[box2d]\\npip3 install pennylane==0.28.0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"%%bash\n",
        "pip3 install swig\n",
        "pip3 install gymnasium[box2d]\n",
        "pip3 install pennylane==0.28.0\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8nUAHWVGiqvo"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dell\\miniconda3\\envs\\gyenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import gym\n",
        "import pennylane as qml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Loa2MqFiqvq"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJX4Ctbqiqvr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7hdJ1gdiqvr",
        "outputId": "59a7d1f1-b1eb-4cf1-aaf9-77eb039d768e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.26.2\n",
            "1.13.1+cpu\n",
            "0.28.0\n"
          ]
        }
      ],
      "source": [
        "print(gym.__version__)\n",
        "print(torch.__version__)\n",
        "print(qml.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gym.envs import box2d\n",
        "\n",
        "FPS = 50\n",
        "SCALE = 30.0  # affects how fast-paced the game is, forces should be adjusted as well\n",
        "\n",
        "MAIN_ENGINE_POWER = 13.0\n",
        "SIDE_ENGINE_POWER = 0.6\n",
        "\n",
        "INITIAL_RANDOM = 1000.0  # Set 1500 to make game harder\n",
        "\n",
        "LANDER_POLY = [(-14, +17), (-17, 0), (-17, -10), (+17, -10), (+17, 0), (+14, +17)]\n",
        "LEG_AWAY = 20\n",
        "LEG_DOWN = 18\n",
        "LEG_W, LEG_H = 2, 8\n",
        "LEG_SPRING_TORQUE = 40\n",
        "\n",
        "SIDE_ENGINE_HEIGHT = 14.0\n",
        "SIDE_ENGINE_AWAY = 12.0\n",
        "\n",
        "VIEWPORT_W = 600\n",
        "VIEWPORT_H = 400\n",
        "\n",
        "class RewardShaped(box2d.lunar_lander.LunarLander):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(RewardShaped, self).__init__(**kwargs)\n",
        "        \n",
        "    def step(self, action):\n",
        "        assert self.lander is not None\n",
        "\n",
        "        # Update wind\n",
        "        assert self.lander is not None, \"You forgot to call reset()\"\n",
        "        if self.enable_wind and not (\n",
        "            self.legs[0].ground_contact or self.legs[1].ground_contact\n",
        "        ):\n",
        "            # the function used for wind is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            wind_mag = (\n",
        "                math.tanh(\n",
        "                    math.sin(0.02 * self.wind_idx)\n",
        "                    + (math.sin(math.pi * 0.01 * self.wind_idx))\n",
        "                )\n",
        "                * self.wind_power\n",
        "            )\n",
        "            self.wind_idx += 1\n",
        "            self.lander.ApplyForceToCenter(\n",
        "                (wind_mag, 0.0),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "            # the function used for torque is tanh(sin(2 k x) + sin(pi k x)),\n",
        "            # which is proven to never be periodic, k = 0.01\n",
        "            torque_mag = math.tanh(\n",
        "                math.sin(0.02 * self.torque_idx)\n",
        "                + (math.sin(math.pi * 0.01 * self.torque_idx))\n",
        "            ) * (self.turbulence_power)\n",
        "            self.torque_idx += 1\n",
        "            self.lander.ApplyTorque(\n",
        "                (torque_mag),\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        if self.continuous:\n",
        "            action = np.clip(action, -1, +1).astype(np.float32)\n",
        "        else:\n",
        "            assert self.action_space.contains(\n",
        "                action\n",
        "            ), f\"{action!r} ({type(action)}) invalid \"\n",
        "\n",
        "        # Engines\n",
        "        tip = (math.sin(self.lander.angle), math.cos(self.lander.angle))\n",
        "        side = (-tip[1], tip[0])\n",
        "        dispersion = [self.np_random.uniform(-1.0, +1.0) / SCALE for _ in range(2)]\n",
        "\n",
        "        m_power = 0.0\n",
        "        if (self.continuous and action[0] > 0.0) or (\n",
        "            not self.continuous and action == 2\n",
        "        ):\n",
        "            # Main engine\n",
        "            if self.continuous:\n",
        "                m_power = (np.clip(action[0], 0.0, 1.0) + 1.0) * 0.5  # 0.5..1.0\n",
        "                assert m_power >= 0.5 and m_power <= 1.0\n",
        "            else:\n",
        "                m_power = 1.0\n",
        "            # 4 is move a bit downwards, +-2 for randomness\n",
        "            ox = tip[0] * (4 / SCALE + 2 * dispersion[0]) + side[0] * dispersion[1]\n",
        "            oy = -tip[1] * (4 / SCALE + 2 * dispersion[0]) - side[1] * dispersion[1]\n",
        "            impulse_pos = (self.lander.position[0] + ox, self.lander.position[1] + oy)\n",
        "            p = self._create_particle(\n",
        "                3.5,  # 3.5 is here to make particle speed adequate\n",
        "                impulse_pos[0],\n",
        "                impulse_pos[1],\n",
        "                m_power,\n",
        "            )  # particles are just a decoration\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * MAIN_ENGINE_POWER * m_power, oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * MAIN_ENGINE_POWER * m_power, -oy * MAIN_ENGINE_POWER * m_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        s_power = 0.0\n",
        "        if (self.continuous and np.abs(action[1]) > 0.5) or (\n",
        "            not self.continuous and action in [1, 3]\n",
        "        ):\n",
        "            # Orientation engines\n",
        "            if self.continuous:\n",
        "                direction = np.sign(action[1])\n",
        "                s_power = np.clip(np.abs(action[1]), 0.5, 1.0)\n",
        "                assert s_power >= 0.5 and s_power <= 1.0\n",
        "            else:\n",
        "                direction = action - 2\n",
        "                s_power = 1.0\n",
        "            ox = tip[0] * dispersion[0] + side[0] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            oy = -tip[1] * dispersion[0] - side[1] * (\n",
        "                3 * dispersion[1] + direction * SIDE_ENGINE_AWAY / SCALE\n",
        "            )\n",
        "            impulse_pos = (\n",
        "                self.lander.position[0] + ox - tip[0] * 17 / SCALE,\n",
        "                self.lander.position[1] + oy + tip[1] * SIDE_ENGINE_HEIGHT / SCALE,\n",
        "            )\n",
        "            p = self._create_particle(0.7, impulse_pos[0], impulse_pos[1], s_power)\n",
        "            p.ApplyLinearImpulse(\n",
        "                (ox * SIDE_ENGINE_POWER * s_power, oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "            self.lander.ApplyLinearImpulse(\n",
        "                (-ox * SIDE_ENGINE_POWER * s_power, -oy * SIDE_ENGINE_POWER * s_power),\n",
        "                impulse_pos,\n",
        "                True,\n",
        "            )\n",
        "\n",
        "        self.world.Step(1.0 / FPS, 6 * 30, 2 * 30)\n",
        "\n",
        "        pos = self.lander.position\n",
        "        vel = self.lander.linearVelocity\n",
        "        state = [\n",
        "            (pos.x - VIEWPORT_W / SCALE / 2) / (VIEWPORT_W / SCALE / 2),\n",
        "            (pos.y - (self.helipad_y + LEG_DOWN / SCALE)) / (VIEWPORT_H / SCALE / 2),\n",
        "            vel.x * (VIEWPORT_W / SCALE / 2) / FPS,\n",
        "            vel.y * (VIEWPORT_H / SCALE / 2) / FPS,\n",
        "            self.lander.angle,\n",
        "            20.0 * self.lander.angularVelocity / FPS,\n",
        "            1.0 if self.legs[0].ground_contact else 0.0,\n",
        "            1.0 if self.legs[1].ground_contact else 0.0,\n",
        "        ]\n",
        "        assert len(state) == 8\n",
        "\n",
        "        reward = 0\n",
        "        shaping = (\n",
        "            -100 * np.sqrt(state[0] * state[0] + state[1] * state[1])\n",
        "            - 150 * np.sqrt(state[2] * state[2] + state[3] * state[3])\n",
        "            - 100 * abs(state[4])\n",
        "            - 20 * abs(state[5])\n",
        "            + 30 * state[6]\n",
        "            + 30 * state[7]\n",
        "        )  # And ten points for legs contact, the idea is if you\n",
        "        # lose contact again after landing, you get negative reward\n",
        "        if self.prev_shaping is not None:\n",
        "            reward = shaping - self.prev_shaping\n",
        "        self.prev_shaping = shaping\n",
        "\n",
        "        reward -= (\n",
        "            m_power * 0.2\n",
        "        )  # less fuel spent is better, about -30 for heuristic landing\n",
        "        reward -= s_power * 0.02\n",
        "\n",
        "        terminated = False\n",
        "        if self.game_over or abs(state[0]) >= 1.0:\n",
        "            terminated = True\n",
        "            reward = -100\n",
        "        #if state[6] == 1 and state[7] == 1 and not self.game_over:\n",
        "        #    self.lander.awake = False\n",
        "        if not self.lander.awake:\n",
        "            terminated = True\n",
        "            reward = +150\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self.render()\n",
        "        return np.array(state, dtype=np.float32), reward, terminated, False, {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gym.envs.registration import register\n",
        "register(\n",
        "    id='Lon', # name given to this new environment\n",
        "    entry_point=__name__+':RewardShaped', # env entry point\n",
        "    kwargs={}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIqq3Mnniqvs"
      },
      "source": [
        "# Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D2qx184Jiqvs"
      },
      "outputs": [],
      "source": [
        "# Define a named tuple for representing transitions in the replay memory\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'done', 'next_state'))\n",
        "\n",
        "# Create a class for the replay memory\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, buffer_size):\n",
        "        # Initialize the replay memory with a specified buffer size\n",
        "        self.buffer_size = buffer_size\n",
        "        self.memory = []  # List to store the transitions\n",
        "        self.index = 0  # Current index in the memory\n",
        "\n",
        "    def push(self, *args):\n",
        "        # Add a new transition to the replay memory\n",
        "        if len(self.memory) < self.buffer_size:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.index] = Transition(*args)\n",
        "        self.index = (self.index + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        # Randomly sample a batch of transitions from the memory\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*(self.memory[idx] for idx in indices))\n",
        "        states = torch.from_numpy(np.array(states)).to(device)\n",
        "        actions = torch.from_numpy(np.array(actions)).to(device)\n",
        "        rewards = torch.from_numpy(np.array(rewards, dtype=np.float32)).to(device)\n",
        "        dones = torch.from_numpy(np.array(dones, dtype=np.int32)).to(device)\n",
        "        next_states = torch.from_numpy(np.array(next_states)).to(device)\n",
        "        return states, actions, rewards, dones, next_states\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the current size of the memory\n",
        "        return len(self.memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7_ZVBzniqvt"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DBf6fhFuiqvt"
      },
      "outputs": [],
      "source": [
        "# Define a class for a Reinforcement Learning Agent\n",
        "class RlAgent:\n",
        "\n",
        "    def __init__(self, net, action_space=None, eps_init=None, eps_decay=None, eps_final=None):\n",
        "        # Initialize the agent with network, action space, and epsilon parameters\n",
        "        self.net = net  # The neural network used by the agent\n",
        "        self.action_space = action_space  # The space of possible actions\n",
        "        self.eps_init = eps_init  # Initial exploration rate\n",
        "        self.eps_decay = eps_decay  # Rate at which exploration decreases\n",
        "        self.eps_final = eps_final  # Final exploration rate\n",
        "        self.epsilon = 0.  # Current exploration rate\n",
        "\n",
        "    def __call__(self, state, device=torch.device('cpu')):\n",
        "        # Define the behavior of the agent when called with a state\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = self.get_random_action()  # Choose a random action\n",
        "        else:\n",
        "            action = self.get_action(state, device)  # Choose an action based on Q-values\n",
        "\n",
        "        return action\n",
        "\n",
        "    def get_random_action(self):\n",
        "        # Choose a random action from the action space\n",
        "        action = self.action_space.sample()\n",
        "        return action\n",
        "\n",
        "    def get_action(self, state, device=torch.device('cpu')):\n",
        "        # Choose an action based on the Q-values from the network\n",
        "        if not isinstance(state, torch.Tensor):\n",
        "            state = torch.tensor(np.array([state]))\n",
        "\n",
        "        if device.type != 'cpu':\n",
        "            state = state.cuda(device)\n",
        "\n",
        "        q_values = self.net.eval()(state)\n",
        "        _, action = torch.max(q_values, dim=1)  # Select the action with the highest Q-value\n",
        "        return int(action.item())\n",
        "\n",
        "    def update_epsilon(self, step):\n",
        "        # Update the exploration rate (epsilon) over time\n",
        "        self.epsilon = max(\n",
        "            self.eps_final, self.eps_final +\n",
        "            (self.eps_init - self.eps_final) *\n",
        "            self.eps_decay**step)\n",
        "        return self.epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T-l5EnLiqvu"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5nSlS5Kkiqvu"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, env, net, target_net, gamma, learning_rate, batch_size,\n",
        "                 exploration_initial_eps, exploration_decay, exploration_final_eps,\n",
        "                 train_freq, target_update_interval, buffer_size, learning_rate_input=None,\n",
        "                 learning_rate_output=None, loss_func='MSE', optim_class='RMSprop',\n",
        "                 device='cpu', logging=False):\n",
        "        \"\"\"\n",
        "        Initialize the trainer for deep reinforcement learning.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): The environment for training.\n",
        "            net (torch.nn.Module): The neural network representing the Q-function.\n",
        "            target_net (torch.nn.Module): The target neural network.\n",
        "            gamma (float): Discount factor for future rewards.\n",
        "            learning_rate (float): Learning rate for optimization.\n",
        "            batch_size (int): Size of mini-batch for training.\n",
        "            exploration_initial_eps (float): Initial exploration epsilon value.\n",
        "            exploration_decay (float): Exploration decay rate.\n",
        "            exploration_final_eps (float): Final exploration epsilon value.\n",
        "            train_freq (int): Frequency of training the network.\n",
        "            target_update_interval (int): Frequency of updating the target network.\n",
        "            buffer_size (int): Size of the replay memory buffer.\n",
        "            learning_rate_input (float, optional): Learning rate for the network input layer.\n",
        "            learning_rate_output (float, optional): Learning rate for the network output layer.\n",
        "            loss_func (str, optional): Loss function for training the network.\n",
        "            optim_class (str, optional): Optimization algorithm.\n",
        "            device (str, optional): Device for training ('auto', 'cpu', or 'cuda:0').\n",
        "            logging (bool, optional): Whether to log training progress.\n",
        "\n",
        "        Raises:\n",
        "            AssertionError: If input arguments do not meet supported options.\n",
        "        \"\"\"\n",
        "        assert loss_func in ['MSE', 'L1', 'SmoothL1'], \"Supported losses: ['MSE', 'L1', 'SmoothL1']\"\n",
        "        assert optim_class in ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta'], \"Supported optimizers: ['SGD', 'RMSprop', 'Adam', 'Adagrad', 'Adadelta']\"\n",
        "        assert device in ['auto', 'cpu', 'cuda:0'], \"Supported devices: ['auto', 'cpu', 'cuda:0']\"\n",
        "\n",
        "        self.env = env\n",
        "        self.net = net\n",
        "        self.target_net = target_net\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.exploration_initial_eps = exploration_initial_eps\n",
        "        self.exploration_decay = exploration_decay\n",
        "        self.exploration_final_eps = exploration_final_eps\n",
        "        self.train_freq = train_freq\n",
        "        self.target_update_interval = target_update_interval\n",
        "        self.buffer_size = buffer_size\n",
        "        self.learning_rate_input = learning_rate_input\n",
        "        self.learning_rate_output = learning_rate_output\n",
        "        self.loss_func = loss_func\n",
        "        self.optim_class = optim_class\n",
        "        self.device = device\n",
        "        self.logging = logging\n",
        "\n",
        "        self.build()\n",
        "        self.reset()\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"\n",
        "        Build the trainer by setting up networks, loss function, optimizer, agent, memory, and loggers.\n",
        "        \"\"\"\n",
        "        if self.device == \"auto\":\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        else:\n",
        "            self.device = torch.device(self.device)\n",
        "        self.net = self.net.to(self.device)\n",
        "        self.target_net = self.target_net.to(self.device)\n",
        "\n",
        "        self.loss_func = getattr(nn, self.loss_func + 'Loss')()\n",
        "\n",
        "        optim_class = getattr(optim, self.optim_class)\n",
        "        params = [{'params': self.net.q_layers.parameters()}]\n",
        "        if hasattr(self.net, 'w_input') and self.net.w_input is not None:\n",
        "            lr_input = self.learning_rate_input if self.learning_rate_input is not None else self.learning_rate\n",
        "            params.append({'params': self.net.w_input, 'lr': lr_input})\n",
        "        if hasattr(self.net, 'w_output') and self.net.w_output is not None:\n",
        "            lr_output = self.learning_rate_output if self.learning_rate_output is not None else self.learning_rate\n",
        "            params.append({'params': self.net.w_output, 'lr': lr_output})\n",
        "        self.opt = optim_class(params, lr=self.learning_rate)\n",
        "\n",
        "        self.agent = RlAgent(self.net, self.env.action_space, self.exploration_initial_eps,\n",
        "                           self.exploration_decay, self.exploration_final_eps)\n",
        "\n",
        "        self.memory = ReplayMemory(self.buffer_size)\n",
        "\n",
        "        if self.logging:\n",
        "            exp_name = datetime.now().strftime(\"DQN-%d_%m_%Y-%H_%M_%S\")\n",
        "            if not os.path.exists('./logs/'):\n",
        "                os.makedirs('./logs/')\n",
        "            self.log_dir = './logs/{}/'.format(exp_name)\n",
        "            os.makedirs(self.log_dir)\n",
        "            self.writer = SummaryWriter(log_dir=self.log_dir)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the trainer's state, initialize episode counts, and pre-fill the replay memory.\n",
        "        \"\"\"\n",
        "        self.global_step = 0\n",
        "        self.episode_count = 0\n",
        "        self.n_actions = self.env.action_space.n\n",
        "        state, _ = self.env.reset(seed=12)\n",
        "        while len(self.memory) < self.buffer_size:\n",
        "            action = self.agent.get_random_action()\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            self.memory.push(state, action, reward, done, next_state)\n",
        "            if terminated or truncated:\n",
        "                state, _ = self.env.reset(seed=12)\n",
        "            else:\n",
        "                state = next_state\n",
        "\n",
        "    def update_net(self):\n",
        "        \"\"\"\n",
        "        Update the neural network by training on a mini-batch of experiences.\n",
        "\n",
        "        Returns:\n",
        "            float: The training loss.\n",
        "        \"\"\"\n",
        "        self.net.train()\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "        states, actions, rewards, dones, next_states = self.memory.sample(self.batch_size, self.device)\n",
        "\n",
        "        state_action_values = self.net(states)\n",
        "        actions = actions.to(torch.int64)\n",
        "        state_action_values = state_action_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_values = self.target_net(next_states)\n",
        "            next_state_values = next_state_values.max(1)[0].detach()\n",
        "\n",
        "        expected_state_action_values = (1 - dones) * next_state_values.to(self.device) * self.gamma + rewards\n",
        "\n",
        "        loss = self.loss_func(state_action_values, expected_state_action_values)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_net(self):\n",
        "        \"\"\"\n",
        "        Update the target neural network by copying the weights from the main network.\n",
        "        \"\"\"\n",
        "        self.target_net.load_state_dict(self.net.state_dict())\n",
        "\n",
        "    def train_step(self):\n",
        "        \"\"\"\n",
        "        Perform a single training step, including taking actions in the environment, updating the network, and updating the target network.\n",
        "\n",
        "        Returns:\n",
        "            dict: Training statistics (steps, loss, reward, epsilon).\n",
        "        \"\"\"\n",
        "        episode_epsilon = self.agent.update_epsilon(self.episode_count)\n",
        "        episode_steps = 0\n",
        "        episode_reward = 0\n",
        "        episode_loss = []\n",
        "        state, _ = self.env.reset(seed=12)\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = self.agent(state, self.device)\n",
        "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            self.memory.push(state, action, reward, done, next_state)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if self.global_step % self.train_freq == 0:\n",
        "                loss = self.update_net()\n",
        "                episode_loss.append(loss)\n",
        "\n",
        "            if self.global_step % self.target_update_interval == 0:\n",
        "                self.update_target_net()\n",
        "\n",
        "            self.global_step += 1\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "\n",
        "        self.episode_count += 1\n",
        "        if len(episode_loss) > 0:\n",
        "            episode_loss = np.mean(episode_loss)\n",
        "        else:\n",
        "            episode_loss = 0.\n",
        "        return {\n",
        "            'steps': episode_steps,\n",
        "            'loss': episode_loss,\n",
        "            'reward': episode_reward,\n",
        "            'epsilon': episode_epsilon\n",
        "        }\n",
        "\n",
        "    def test_step(self, n_eval_episodes):\n",
        "        \"\"\"\n",
        "        Perform a test step by evaluating the agent's performance over multiple episodes.\n",
        "\n",
        "        Args:\n",
        "            n_eval_episodes (int): Number of episodes to evaluate.\n",
        "\n",
        "        Returns:\n",
        "            dict: Evaluation statistics (average steps and average reward).\n",
        "        \"\"\"\n",
        "        episode_steps = []\n",
        "        episode_reward = []\n",
        "\n",
        "        for _ in range(n_eval_episodes):\n",
        "            state, _ = self.env.reset(seed=12)\n",
        "            done = False\n",
        "            episode_steps.append(0)\n",
        "            episode_reward.append(0)\n",
        "            while not done:\n",
        "                action = self.agent.get_action(state, self.device)\n",
        "                next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                episode_steps[-1] += 1\n",
        "                episode_reward[-1] += reward\n",
        "\n",
        "        episode_steps = np.mean(episode_steps)\n",
        "        episode_reward = np.mean(episode_reward)\n",
        "        return {'steps': episode_steps, 'reward': episode_reward}\n",
        "\n",
        "    def learn(self, total_episodes, n_eval_episodes=5, log_train_freq=-1, log_eval_freq=-1, log_ckp_freq=-1):\n",
        "        \"\"\"\n",
        "        Perform the main training loop, including training and evaluation.\n",
        "\n",
        "        Args:\n",
        "            total_episodes (int): Total number of training episodes.\n",
        "            n_eval_episodes (int, optional): Number of episodes to use for evaluation.\n",
        "            log_train_freq (int, optional): Frequency of logging training statistics.\n",
        "            log_eval_freq (int, optional): Frequency of logging evaluation statistics.\n",
        "            log_ckp_freq (int, optional): Frequency of saving checkpoints.\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        postfix_stats = {}\n",
        "        with tqdm(range(total_episodes), desc=\"DQN\", unit=\"episode\") as tepisodes:\n",
        "            for t in tepisodes:\n",
        "                train_stats = self.train_step()\n",
        "                postfix_stats['train/reward'] = train_stats['reward']\n",
        "                postfix_stats['train/steps'] = train_stats['steps']\n",
        "\n",
        "                if t % log_eval_freq == 0:\n",
        "                    test_stats = self.test_step(n_eval_episodes)\n",
        "                    postfix_stats['test/reward'] = test_stats['reward']\n",
        "                    postfix_stats['test/steps'] = test_stats['steps']\n",
        "\n",
        "                if self.logging and (t % log_train_freq == 0):\n",
        "                    for key, item in train_stats.items():\n",
        "                        self.writer.add_scalar('train/' + key, item, t)\n",
        "\n",
        "                if self.logging and (t % log_eval_freq == 0):\n",
        "                    for key, item in test_stats.items():\n",
        "                        self.writer.add_scalar('test/' + key, item, t)\n",
        "\n",
        "                if self.logging and (t % log_ckp_freq == 0):\n",
        "                    torch.save(self.net.state_dict(), self.log_dir + 'episode_{}.pt'.format(t))\n",
        "\n",
        "                tepisodes.set_postfix(postfix_stats)\n",
        "\n",
        "            if self.logging and (log_ckp_freq > 0):\n",
        "                torch.save(self.net.state_dict(), self.log_dir + 'episode_final.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhzO3HSBiqvv"
      },
      "source": [
        "# Quantum Circuits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2-dxvD2hiqvv"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "from torch.nn.init import normal_\n",
        "\n",
        "def get_circuit(n_qubits, n_layers, data_reupload):\n",
        "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "    dimensions = {\n",
        "        \"x_weights\": (n_layers, n_qubits),\n",
        "        \"z_weights\": (n_layers, n_qubits)\n",
        "    }\n",
        "\n",
        "    @qml.qnode(dev, interface='torch')\n",
        "    def circuit(inputs, x_weights, z_weights):\n",
        "        for layer_idx in range(n_layers):\n",
        "            if (layer_idx == 0) or data_reupload:\n",
        "                for wire in range(n_qubits):\n",
        "                    qml.RX(inputs[wire], wires=wire)\n",
        "            for wire, x_weight in enumerate(x_weights[layer_idx]):\n",
        "                qml.RX(x_weight, wires=wire)\n",
        "            for wire, z_weight in enumerate(z_weights[layer_idx]):\n",
        "                qml.RZ(z_weight, wires=wire)\n",
        "            for wire in range(n_qubits):\n",
        "                qml.CNOT(wires=[wire, (wire + 1) % n_qubits])\n",
        "        return [\n",
        "            qml.expval(qml.PauliZ(6) @ qml.PauliZ(7)),\n",
        "            qml.expval(qml.PauliZ(0) @ qml.PauliZ(5)),\n",
        "            qml.expval(qml.PauliZ(1) @ qml.PauliZ(3)),\n",
        "            qml.expval(qml.PauliZ(2) @ qml.PauliZ(4))\n",
        "        ]\n",
        "\n",
        "    model = qml.qnn.TorchLayer(circuit, dimensions)\n",
        "    return model\n",
        "\n",
        "class QuantumNet(Module):\n",
        "    def __init__(self, n_layers, w_input, w_output, data_reupload):\n",
        "        super(QuantumNet, self).__init__()\n",
        "        self.n_qubits = 8\n",
        "        self.n_actions = 4\n",
        "        self.data_reupload = data_reupload\n",
        "        self.q_layers = get_circuit(n_qubits=self.n_qubits, n_layers=n_layers, data_reupload=data_reupload)\n",
        "        self.w_input = Parameter(torch.Tensor(self.n_qubits)) if w_input else None\n",
        "        if self.w_input is not None:\n",
        "            normal_(self.w_input, mean=0.)\n",
        "        self.w_output = Parameter(torch.Tensor(self.n_actions)) if w_output else None\n",
        "        if self.w_output is not None:\n",
        "            normal_(self.w_output, mean=100.)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.w_input is not None:\n",
        "            inputs = inputs * self.w_input\n",
        "        inputs = torch.atan(inputs)\n",
        "        outputs = self.q_layers(inputs)\n",
        "        #outputs = (1 + outputs) / 2\n",
        "        #print(f\"Before: {outputs}\")\n",
        "        outputs = -1 + (outputs - outputs.min()) * (2) / (outputs.max() - outputs.min())\n",
        "        #print(f\"After: {outputs}\")\n",
        "        if self.w_output is not None:\n",
        "            outputs = outputs * self.w_output\n",
        "        else:\n",
        "            outputs = 100 * outputs\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Jd8fEvmNiqvw"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "\n",
        "def saveanimation(frames,address):\n",
        "    imageio.mimwrite(address, frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAgGf1mXiqvw"
      },
      "source": [
        "# Tester Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1fgFpq0diqvw"
      },
      "outputs": [],
      "source": [
        "def tester(env, agent, episodes):\n",
        "    episode_steps = []\n",
        "    episode_reward = []\n",
        "    #frames = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state,_ = env.reset()\n",
        "        done = False\n",
        "        episode_steps.append(0)\n",
        "        episode_reward.append(0)\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            #frames.append(env.render())\n",
        "            next_state, reward, terminated, truncated ,_= env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            episode_steps[-1] += 1\n",
        "            episode_reward[-1] += reward\n",
        "        #frames.append(env.render())\n",
        "\n",
        "    env.close()\n",
        "    print(episode_steps, episode_reward)\n",
        "    episode_steps = np.mean(episode_steps)\n",
        "    episode_reward = np.mean(episode_reward)\n",
        "    #saveanimation(frames)\n",
        "    return {'steps': episode_steps, 'reward': episode_reward}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O7BSzUsPiqvw"
      },
      "outputs": [],
      "source": [
        "n_layers = 5\n",
        "gamma = 0.9999\n",
        "w_input = True\n",
        "w_output = True\n",
        "lr = 0.005\n",
        "lr_input = 0.001\n",
        "lr_output = 0.001\n",
        "batch_size = 16\n",
        "eps_init = 1.\n",
        "eps_decay = 0.99\n",
        "eps_min = 0.05\n",
        "train_freq = 10\n",
        "target_freq = 30\n",
        "memory = 25000\n",
        "data_reupload = True\n",
        "loss = 'SmoothL1'\n",
        "optimizer = 'Adam'\n",
        "total_episodes = 500\n",
        "n_eval_episodes = 5\n",
        "logging = True\n",
        "log_train_freq = 1\n",
        "log_eval_freq = 20\n",
        "log_ckp_freq = 20\n",
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "680fbfa131f94618bf812189380e7ec7",
            "d24ea73ffd184a2b97eb12df94b9d7ce",
            "3d6f5c59d06f4fbaa63c35cd0c784bef",
            "312ccb93e16d4eb8a0124f5928834fca",
            "46f8fb9d0c7847358dd27a5f6100e0d4",
            "b65d45de6629426b924fd2fe528b904a",
            "45689ad7eaba4004b7499f6abe91cea9",
            "ff6c2427869243fba87f800bfbbfd563",
            "0ff410e1c469464cb80f390959e89578",
            "d3119e8fa4d54c09b80cc9c517b40bb0",
            "f626c3e762374076acb43b32bf3e1485"
          ]
        },
        "id": "NUtqzeJpiqvw",
        "outputId": "8aab2d40-c27f-4348-c6a5-b4c917329206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DQN: 100%|██████████| 500/500 [8:55:59<00:00, 64.32s/episode, train/reward=-637, train/steps=57, test/reward=-26.9, test/steps=71]    \n"
          ]
        }
      ],
      "source": [
        "# Environment\n",
        "env_name = 'Lon'\n",
        "env = gym.make(env_name)\n",
        "#print(env)\n",
        "\n",
        "# Networks\n",
        "net = QuantumNet(n_layers, w_input, w_output, data_reupload)\n",
        "target_net = QuantumNet(n_layers, w_input, w_output, data_reupload)\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(env,\n",
        "                  net,\n",
        "                  target_net,\n",
        "                  gamma=gamma,\n",
        "                  learning_rate=lr,\n",
        "                  batch_size=batch_size,\n",
        "                  exploration_initial_eps=eps_init,\n",
        "                  exploration_decay=eps_decay,\n",
        "                  exploration_final_eps=eps_min,\n",
        "                  train_freq=train_freq,\n",
        "                  target_update_interval=target_freq,\n",
        "                  buffer_size=memory,\n",
        "                  learning_rate_input=lr_input,\n",
        "                  learning_rate_output=lr_output,\n",
        "                  loss_func=loss,\n",
        "                  optim_class=optimizer,\n",
        "                  device=device,\n",
        "                  logging=logging)\n",
        "\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir=logs/\n",
        "\n",
        "trainer.learn(total_episodes,\n",
        "              n_eval_episodes=n_eval_episodes,\n",
        "              log_train_freq=log_train_freq,\n",
        "              log_eval_freq=log_eval_freq,\n",
        "              log_ckp_freq=log_ckp_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oS405Acziqvx"
      },
      "outputs": [],
      "source": [
        "torch.save(trainer.net.state_dict(), 'net5.pt')\n",
        "torch.save(trainer.target_net.state_dict(), 'tar_net5.pt')\n",
        "\n",
        "import pickle\n",
        "\n",
        "file = open('agents5', 'wb')\n",
        "arr = [trainer.exploration_decay, trainer.exploration_initial_eps, trainer.exploration_final_eps]\n",
        "\n",
        "pickle.dump(arr, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tester(env, agent, episodes):\n",
        "    episode_steps = []\n",
        "    episode_reward = []\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state,_ = env.reset()\n",
        "        done = False\n",
        "        episode_steps.append(0)\n",
        "        episode_reward.append(0)\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            frames.append(env.render())\n",
        "            next_state, reward, terminated, truncated ,_= env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "            episode_steps[-1] += 1\n",
        "            episode_reward[-1] += reward\n",
        "        frames.append(env.render())\n",
        "\n",
        "    env.close()\n",
        "    print(episode_steps, episode_reward)\n",
        "    episode_steps = np.mean(episode_steps)\n",
        "    episode_reward = np.mean(episode_reward)\n",
        "    saveanimation(frames,\"./ren2.gif\")\n",
        "    return {'steps': episode_steps, 'reward': episode_reward}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Pmb-_MS2iqvx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[68, 58] [-234.01898547231926, -330.2962123158827]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'steps': 63.0, 'reward': -282.157598894101}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_env = gym.make(\"Lon\", render_mode = \"rgb_array\")\n",
        "tester(new_env, trainer.agent, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gyenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ff410e1c469464cb80f390959e89578": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "312ccb93e16d4eb8a0124f5928834fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3119e8fa4d54c09b80cc9c517b40bb0",
            "placeholder": "​",
            "style": "IPY_MODEL_f626c3e762374076acb43b32bf3e1485",
            "value": " 15/500 [25:27&lt;13:10:08, 97.75s/episode, train/reward=-355, train/steps=80, test/reward=4.77, test/steps=68]"
          }
        },
        "3d6f5c59d06f4fbaa63c35cd0c784bef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff6c2427869243fba87f800bfbbfd563",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ff410e1c469464cb80f390959e89578",
            "value": 15
          }
        },
        "45689ad7eaba4004b7499f6abe91cea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46f8fb9d0c7847358dd27a5f6100e0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "680fbfa131f94618bf812189380e7ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d24ea73ffd184a2b97eb12df94b9d7ce",
              "IPY_MODEL_3d6f5c59d06f4fbaa63c35cd0c784bef",
              "IPY_MODEL_312ccb93e16d4eb8a0124f5928834fca"
            ],
            "layout": "IPY_MODEL_46f8fb9d0c7847358dd27a5f6100e0d4"
          }
        },
        "b65d45de6629426b924fd2fe528b904a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d24ea73ffd184a2b97eb12df94b9d7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b65d45de6629426b924fd2fe528b904a",
            "placeholder": "​",
            "style": "IPY_MODEL_45689ad7eaba4004b7499f6abe91cea9",
            "value": "DQN:   3%"
          }
        },
        "d3119e8fa4d54c09b80cc9c517b40bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f626c3e762374076acb43b32bf3e1485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff6c2427869243fba87f800bfbbfd563": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
